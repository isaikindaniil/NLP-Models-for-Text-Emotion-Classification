# NLP-Models-for-Text-Emotion-Classification

–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç üëã 

–î–∞–Ω–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π NLP –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤ —Ç–µ–∫—Å—Ç–µ. –û–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–∑—ã–≤–æ–≤ –∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞ –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

## –û –ø—Ä–æ–µ–∫—Ç–µ

–ó–∞ –æ—Å–Ω–æ–≤—É –±—ã–ª –≤–∑—è—Ç –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å —Å–∞–π—Ç–∞ kaggle (https://www.kaggle.com/datasets/simaanjali/emotion-analysis-based-on-text), –æ–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –±–æ–ª–µ–µ —á–µ–º 800 —Ç—ã—Å—è—á —Å—Ç—Ä–æ–∫, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–∞–Ω–Ω—ã—Ö —ç–º–æ—Ü–∏–π: 
- –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (neutral)
- –≤—ã—Ä–∞–∂–µ–Ω–∏—è –ª—é–±–≤–∏ (love)
- –ø–µ—á–∞–ª—å (sadness)
- —á—É–≤—Å—Ç–≤–æ –æ–±–ª–µ–≥—á–µ–Ω–∏—è (relief)
- –Ω–µ–Ω–∞–≤–∏—Å—Ç—å (hate)
- –∑–ª–æ—Å—Ç—å (anger)
- –≤–µ—Å–µ–ª—å–µ/—Ä–∞–¥–æ—Å—Ç—å (fun)
- —ç–Ω—Ç—É–∑–∏–∞–∑–º (enthusiasm)
- —É–¥–∏–≤–ª–µ–Ω–∏–µ (surprise)
- –ø—É—Å—Ç–æ—Ç–∞ (empty)
- –±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ (worry)
- —Å–∫—É–∫–∞ (boredom)

## –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏

–í –ø—Ä–æ–µ–∫—Ç–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –¥–≤–µ –º–æ–¥–µ–ª–∏:

1. **emotion_classification_model** - NLP –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.
   - –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: ~99%
   - –ë—ã—Å—Ç—Ä–∞—è —Ä–∞–±–æ—Ç–∞ –Ω–∞ CPU
   - –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏

2. **bert_emotion_model** - NLP –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–æ—Ç–æ–≤–æ–π BERT –º–æ–¥–µ–ª–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π (fine-tuning) –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–∞—Ö.
   - –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: ~98%
   - –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ç–µ–∫—Å—Ç–∞
   - –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è

- `emotion_class_models.ipynb` - Jupyter notebook —Å –ø–æ–ª–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- `example.ipynb` - –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤
- `emotion_classification_model.pkl` - –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è TF-IDF + LogReg –º–æ–¥–µ–ª—å
- `bert_emotion_model/` - –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π BERT –º–æ–¥–µ–ª—å—é
- `README.md` - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞

## –û –Ω–æ—É—Ç–±—É–∫–µ emotion_class_models.ipynb

–û—Å–Ω–æ–≤–Ω–æ–π –Ω–æ—É—Ç–±—É–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —ç–º–æ—Ü–∏–π –∏ –≤–∫–ª—é—á–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã:

1. **–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**:
   - –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –æ–∫–æ–ª–æ 840,000 –∑–∞–ø–∏—Å–µ–π
   - –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–º–æ—Ü–∏–π (–Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è "neutral" - 674,538 –ø—Ä–∏–º–µ—Ä–æ–≤)
   - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é seaborn –∏ matplotlib

2. **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞**:
   - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
   - –£–¥–∞–ª–µ–Ω–∏–µ URL-–∞–¥—Ä–µ—Å–æ–≤, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, —Ü–∏—Ñ—Ä
   - –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (—á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤, –Ω–µ –Ω–µ—Å—É—â–∏—Ö —Å–º—ã—Å–ª–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏)
   - –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤ –∫ –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º–µ)

3. **–ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–æ–≤**:
   - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —ç–º–æ—Ü–∏–∏
   - –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è —ç–º–æ—Ü–∏–∏ "hate" –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –æ–∫–∞–∑–∞–ª–∏—Å—å "feel", "hate", "feeling", "hated"

4. **–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –º–æ–¥–µ–ª–µ–π**:
   - TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
   - Fine-tuning BERT –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
   - –û—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏, F1-score –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫

5. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è**:
   - –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
   - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

–í—ã –º–æ–∂–µ—Ç–µ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∫–∞–∂–¥–æ–π –∏–∑ –º–æ–¥–µ–ª–∏, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–º–∫–∞—Ö —Ç–µ—Ç—Ä–∞–¥–∫–∏ (`emotion_class_models.ipynb`). –ö–æ–¥ –Ω–∞–ø–∏—Å–∞–Ω —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –ª–µ–≥–∫–æ –º–æ–∂–Ω–æ –±—ã–ª–æ –≤–Ω–µ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∏–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è (–∞ —Ç–∞–∫–∂–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏) –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –¥—Ä—É–≥–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, –∫ –ø—Ä–∏–º–µ—Ä—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤.

–í —Ç–µ—Ç—Ä–∞–¥–∫–µ `example.ipynb` –≤—ã –º–æ–∂–µ—Ç–µ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –ø—Ä–∏–º–µ—Ä–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–π, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Ç–µ–∫—Å—Ç–∞—Ö.


## –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

–î–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–µ–∫—Ç–æ–º –≤–∞–º –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è:

- Python 3.8+
- pandas
- numpy
- scikit-learn
- torch
- transformers
- nltk
- joblib
- matplotlib
- seaborn

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

`pip install -r requirements.txt`

### –°–æ–¥–µ—Ä–∂–∏–º–æ–µ requirements.txt

`pandas==2.0.0
numpy==1.24.3
scikit-learn==1.2.2
torch==2.0.0
transformers==4.28.1
nltk==3.8.1
joblib==1.2.0
matplotlib==3.7.1
seaborn==0.12.2`


## –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

–ë—É–¥—É —Ä–∞–¥ –ª—é–±–æ–º—É, –∫—Ç–æ –∂–µ–ª–∞–µ—Ç –≤–Ω–µ—Å—Ç–∏ –≤–∫–ª–∞–¥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –¥–∞–Ω–Ω–æ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –∞ —Ç–∞–∫–∂–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.

–í—ã –º–æ–∂–µ—Ç–µ:
- –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –Ω–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤
- –£–ª—É—á—à–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
- –î–æ–±–∞–≤–∏—Ç—å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π

## –õ–∏—Ü–µ–Ω–∑–∏—è

[MIT License](LICENSE)

-----------------------------------------------------------------
Hello üëã 

This repository contains NLP models that help classify emotions in text. They can be useful for analyzing customer reviews and feedback, marketing and communication, and content moderation.

## About the Project

The models were built using an open dataset from Kaggle (https://www.kaggle.com/datasets/simaanjali/emotion-analysis-based-on-text), which consists of more than 800,000 lines with the following emotion labels: 
- neutral
- love
- sadness
- relief
- hate
- anger
- fun
- enthusiasm
- surprise
- empty
- worry
- boredom

## Implemented Models

Two models have been implemented:

1. **emotion_classification_model** - NLP classification model based on TF-IDF text vectorization with a classic logistic regression algorithm.
   - Test accuracy: ~99%
   - Fast performance on CPU
   - Compact model size

2. **bert_emotion_model** - NLP classification model based on a pre-trained BERT model fine-tuned for emotion classification in texts.
   - Test accuracy: ~98%
   - Deeper understanding of text context
   - Requires more computational resources but works more effectively with new data

## Repository Structure

- `emotion_class_models.ipynb` - Jupyter notebook with the full process of training models and data preprocessing
- `example.ipynb` - Example of using ready-made models for text classification
- `emotion_classification_model.pkl` - Saved TF-IDF + LogReg model
- `bert_emotion_model/` - Directory with the saved BERT model
- `README.md` - Project documentation

## About the emotion_class_models.ipynb Notebook

The main notebook contains the entire process of creating emotion classification models and includes the following stages:

1. **Data Loading and Exploration**:
   - Loading the dataset with approximately 840,000 records
   - Analysis of emotion distribution (most frequent is "neutral" - 674,538 examples)
   - Data visualization using seaborn and matplotlib

2. **Text Preprocessing**:
   - Converting text to lowercase
   - Removing URLs, special characters, and digits
   - Removing stop words (frequent words that don't carry meaning)
   - Lemmatization (reducing words to their base form)

3. **Token Analysis**:
   - Extracting the most frequently occurring words for each emotion
   - For example, for the emotion "hate" the most common words were "feel", "hate", "feeling", "hated"

4. **Model Development**:
   - TF-IDF text vectorization and training logistic regression
   - Fine-tuning BERT model on this dataset
   - Evaluation and comparison of models using accuracy metrics, F1-score, and confusion matrix visualization

5. **Testing and Validation**:
   - Testing models on individual examples
   - Interactive interface for classification testing

## Usage

You can independently study the process of creating each model, as well as data preprocessing, in the notebook (`emotion_class_models.ipynb`). The code is written in a way that makes it easy to make changes and use it to create (as well as modify and improve the performance of) new classification models on other datasets, for example, to create a text toxicity classifier.

In the notebook `example.ipynb`, you can see examples of using the ready-made models for your own purposes, and test them on external texts.


## Technical Requirements

To work with this project you will need:

- Python 3.8+
- pandas
- numpy
- scikit-learn
- torch
- transformers
- nltk
- joblib
- matplotlib
- seaborn

### Installing Dependencies

`pip install -r requirements.txt`


### Contents of requirements.txt

`pandas==2.0.0
numpy==1.24.3
scikit-learn==1.2.2
torch==2.0.0
transformers==4.28.1
nltk==3.8.1
joblib==1.2.0
matplotlib==3.7.1
seaborn==0.12.2`


## Contributing to the Project

I welcome anyone who wishes to contribute to the development of this repository, as well as any feedback on using these models.

You can:
- Add support for new languages
- Improve text preprocessing
- Implement additional models
- Add a web interface to demonstrate capabilities

## License

[MIT License](LICENSE)

