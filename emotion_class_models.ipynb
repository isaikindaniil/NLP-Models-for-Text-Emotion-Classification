{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64084a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0345846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"emotion_sentimen_dataset.csv\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df['Emotion'].value_counts())\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92dacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+|\\#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(df[['text','cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c088c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "df['preprocessed_text'] = df['cleaned_text'].apply(preprocess_text)\n",
    "print(df[['text', 'cleaned_text', 'preprocessed_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=df, x='Emotion', order=df['Emotion'].value_counts().index)\n",
    "plt.title('Распределение эмоций в датасете')\n",
    "plt.xlabel('Эмоция')\n",
    "plt.ylabel('Количество')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769378ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus,n=10):\n",
    "    words = ' '.join(corpus).split()\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "emotion_words = {}\n",
    "\n",
    "for emotion in df['Emotion'].unique():\n",
    "    emotion_text = df[df['Emotion'] == emotion]['preprocessed_text']\n",
    "    emotion_words[emotion] = get_top_n_words(emotion_text, n=10)\n",
    "\n",
    "for emotion, words in emotion_words.items():\n",
    "    print(f\"Most popular words for emotion - {emotion}:\")\n",
    "    for word, count in words:\n",
    "        print(f\"{word}: {count}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a317cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed_emotion_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e0c0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6592fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_emotion_dataset.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['preprocessed_text']\n",
    "y = df['Emotion']\n",
    "\n",
    "print(f\"Количество NaN в данных: {X.isna().sum()}\")\n",
    "X = X.fillna('')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features = 5000,\n",
    "    min_df = 5,\n",
    "    max_df = 0.8\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ba9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "    max_iter = 1000,\n",
    "    random_state = 42,\n",
    "    class_weight = 'balanced'\n",
    ")\n",
    "\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_logreg = logreg.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nРезультаты для Логистической Регрессии:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_logreg):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715183c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_proba = logreg.predict_proba(X_test_tfidf)\n",
    "\n",
    "roc_auc_ovr = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "print(f\"\\nROC AUC (OvR): {roc_auc_ovr:.4f}\")\n",
    "\n",
    "roc_auc_macro = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "print(f\"ROC AUC (macro): {roc_auc_macro:.4f}\")\n",
    "\n",
    "roc_auc_weighted = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "print(f\"ROC AUC (weighted): {roc_auc_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbbf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "unique_emotions = sorted(y.unique())\n",
    "cm = confusion_matrix(y_test, y_pred_logreg, labels=unique_emotions)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=unique_emotions, yticklabels=unique_emotions)\n",
    "plt.title('Нормализованная матрица ошибок (Логистическая Регрессия)')\n",
    "plt.xlabel('Предсказанные метки')\n",
    "plt.ylabel('Истинные метки')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param_distr = {\n",
    "    'C': [0.1,1,10],\n",
    "    'solver': ['liblinear','saga'],\n",
    "    'penalty': ['l1','l2']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    param_distributions=param_distr,\n",
    "    n_iter = 5,\n",
    "    cv = 5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs= -1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_tfidf,y_train)\n",
    "print(\"\\nЛучшие параметры для Логистической Регрессии:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"Лучший F1-macro: {random_search.best_score_:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "best_logreg = LogisticRegression(\n",
    "    **random_search.best_params_,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    class_weight = 'balanced'\n",
    ")\n",
    "best_logreg.fit(X_train_tfidf,y_train)\n",
    "y_pred_best_logreg = best_logreg.predict(X_test_tfidf)\n",
    "print(\"\\nРезультаты для оптимизированной Логистической Регрессии:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best_logreg):.4f}\")\n",
    "print(classification_report(y_test, y_pred_best_logreg))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vectorizer),\n",
    "    ('model', logreg)\n",
    "])\n",
    "\n",
    "\n",
    "joblib.dump(pipeline, 'emotion_classification_model.pkl')\n",
    "print(\"Модель сохранена в файл 'emotion_classification_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d211c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    \"I'm so happy to see you today!\",\n",
    "    \"This makes me really angry and frustrated.\",\n",
    "    \"I'm feeling quite sad and depressed after what happened.\",\n",
    "    \"I'm worried about the upcoming exam.\",\n",
    "    \"I absolutely hate when people do this to me.\",\n",
    "    \"I love spending time with my family.\",\n",
    "    \"I'm feeling neutral about the whole situation.\"\n",
    "]\n",
    "\n",
    "for text in test_examples:\n",
    "    emotion = pipeline.predict([text])[0]\n",
    "    \n",
    "    if hasattr(pipeline['model'], 'predict_proba'):\n",
    "        probas = pipeline.predict_proba([text])[0]\n",
    "        emotion_probas = dict(zip(pipeline['model'].classes_, probas))\n",
    "        top_emotions = sorted(emotion_probas.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        \n",
    "        print(f\"Текст: '{text}'\")\n",
    "        print(f\"Предсказанная эмоция: {emotion}\")\n",
    "        print(\"Топ-3 вероятностей:\")\n",
    "        for emotion_name, prob in top_emotions:\n",
    "            print(f\"  - {emotion_name}: {prob:.4f} ({prob*100:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec84203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emotion(text):\n",
    "    emotion = pipeline.predict([text])[0]\n",
    "    \n",
    "    print(f\"Текст: '{text}'\")\n",
    "    print(f\"Определенная эмоция: {emotion}\")\n",
    "    \n",
    "    if hasattr(pipeline['model'], 'predict_proba'):\n",
    "        probas = pipeline.predict_proba([text])[0]\n",
    "        emotion_probas = dict(zip(pipeline['model'].classes_, probas))\n",
    "        top_emotions = sorted(emotion_probas.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        \n",
    "        print(\"Топ-3 вероятных эмоций:\")\n",
    "        for emotion_name, prob in top_emotions:\n",
    "            print(f\"  - {emotion_name}: {prob:.4f} ({prob*100:.1f}%)\")\n",
    "    \n",
    "    return emotion\n",
    "\n",
    "print(\"\\nВведите текст для анализа эмоций (для выхода введите 'выход'):\")\n",
    "while True:\n",
    "    user_input = input(\"> \")\n",
    "    if user_input.lower() == 'выход':\n",
    "        break\n",
    "    analyze_emotion(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b25579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_emotion_dataset.csv')\n",
    "\n",
    "labels = df['Emotion'].unique()\n",
    "num_labels = len(labels)\n",
    "\n",
    "labels = df['Emotion'].unique()\n",
    "label_dict = {}\n",
    "for index,label in enumerate(labels):\n",
    "    label_dict[label] = index\n",
    "\n",
    "df['label'] = df['Emotion'].map(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89300682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataset(df, reduction_factor=8):\n",
    "\n",
    "    original_size = len(df)\n",
    "    reduced_df = pd.DataFrame()\n",
    "\n",
    "    class_counts = df['Emotion'].value_counts()\n",
    "    print(\"Исходное распределение классов:\")\n",
    "    print(class_counts)\n",
    "    \n",
    "    rare_threshold = original_size * 0.01\n",
    "    \n",
    "    for emotion, count in class_counts.items():\n",
    "        emotion_df = df[df['Emotion'] == emotion]\n",
    "\n",
    "        if count < rare_threshold:\n",
    "            sample_size = min(count, int(count * 0.5))\n",
    "        else:\n",
    "            sample_size = max(500, int(count / reduction_factor))\n",
    "        \n",
    "        sampled = emotion_df.sample(n=sample_size, random_state=42)\n",
    "        reduced_df = pd.concat([reduced_df, sampled])\n",
    "\n",
    "    reduced_df = reduced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Исходный размер: {original_size}, Новый размер: {len(reduced_df)}\")\n",
    "    print(\"Новое распределение классов:\")\n",
    "    print(reduced_df['Emotion'].value_counts())\n",
    "    \n",
    "    return reduced_df\n",
    "\n",
    "df = reduce_dataset(df, reduction_factor=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45226865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['preprocessed_text'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['preprocessed_text'], df['label'], test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "def tokenize_text(texts, tokenizer, max_len=MAX_LEN):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_len,\n",
    "            pad_to_max_length = True,\n",
    "            truncation = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt',\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "train_inputs, train_masks = tokenize_text(X_train, tokenizer)\n",
    "test_inputs, test_masks = tokenize_text(X_test, tokenizer)\n",
    "\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "test_labels = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03bd8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, inputs, masks, labels):\n",
    "        self.inputs = inputs\n",
    "        self.masks = masks\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx],\n",
    "            'attention_mask': self.masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "    \n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = EmotionDataset(train_inputs,train_masks,train_labels)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = EmotionDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используется устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce280e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\",  \n",
    "    num_labels=num_labels, \n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,\n",
    "                  eps=1e-8)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323adab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def train_model(model, train_dataloader, optimizer, scheduler, device, epochs=4):\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(f\"\\n======== Эпоха {epoch_i + 1} / {epochs} ========\")\n",
    "        print('Обучение...')\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print(f'  Батч {step}  из  {len(train_dataloader)}. Прошло: {elapsed}')\n",
    "\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(f\"\\n  Среднее значение потери: {avg_train_loss:.2f}\")\n",
    "        print(f\"  Время обучения: {training_time}\")\n",
    "\n",
    "        print(\"\\nОценка модели на тестовом наборе...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    labels=b_labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                \n",
    "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "        avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "        avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "        \n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        print(f\"  Точность: {avg_val_accuracy:.2f}\")\n",
    "        print(f\"  Потери валидации: {avg_val_loss:.2f}\")\n",
    "        print(f\"  Время валидации: {validation_time}\")\n",
    "        \n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\"\\nОбучение завершено!\")\n",
    "    print(f\"Общее время обучения: {format_time(time.time() - total_t0)}\")\n",
    "    \n",
    "    return training_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    device, \n",
    "    epochs=4\n",
    ")\n",
    "\n",
    "stats_df = pd.DataFrame(training_stats)\n",
    "stats_df = stats_df.set_index('epoch')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(stats_df['Training Loss'], 'b-o', label='Training')\n",
    "plt.plot(stats_df['Valid. Loss'], 'g-o', label='Validation')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(stats_df['Valid. Accur.'], 'r-o')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './bert_emotion_model/'\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Модель сохранена в {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72541fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion_bert(text, model, tokenizer, label_map, device):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+|\\#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.lower()\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probs = torch.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    pred_class = np.argmax(probs)\n",
    "\n",
    "    emotion = label_map[pred_class]\n",
    "\n",
    "    emotion_probs = {label_map[i]: float(probs[i]) for i in range(len(probs))}\n",
    "\n",
    "    emotion_probs = dict(sorted(emotion_probs.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return {\n",
    "        'emotion': emotion,\n",
    "        'probabilities': emotion_probs\n",
    "    }\n",
    "\n",
    "label_map_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "test_examples = [\n",
    "    \"I'm so happy to see you today!\",\n",
    "    \"This makes me really angry and frustrated.\",\n",
    "    \"I'm feeling quite sad and depressed after what happened.\",\n",
    "    \"I'm worried about the upcoming exam.\",\n",
    "    \"I absolutely hate when people do this to me.\",\n",
    "    \"I love spending time with my family.\",\n",
    "    \"I'm feeling neutral about the whole situation.\"\n",
    "]\n",
    "\n",
    "for text in test_examples:\n",
    "    result = predict_emotion_bert(text, model, tokenizer, label_map_inverse, device)\n",
    "    \n",
    "    print(f\"Текст: '{text}'\")\n",
    "    print(f\"Предсказанная эмоция: {result['emotion']}\")\n",
    "    print(\"Топ-3 вероятностей:\")\n",
    "    top3 = list(result['probabilities'].items())[:3]\n",
    "    for emotion, prob in top3:\n",
    "        print(f\"  - {emotion}: {prob:.4f} ({prob*100:.1f}%)\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84900fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emotion_interactive():\n",
    "    print(\"\\nВведите текст для анализа эмоций (для выхода введите 'выход'):\")\n",
    "    while True:\n",
    "        user_input = input(\"> \")\n",
    "        if user_input.lower() == 'выход':\n",
    "            break\n",
    "        result = predict_emotion_bert(user_input, model, tokenizer, label_map_inverse, device)\n",
    "        \n",
    "        print(f\"Определенная эмоция: {result['emotion']}\")\n",
    "        print(\"Топ-3 вероятных эмоций:\")\n",
    "        top3 = list(result['probabilities'].items())[:3]\n",
    "        for emotion, prob in top3:\n",
    "            print(f\"  - {emotion}: {prob:.4f} ({prob*100:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "analyze_emotion_interactive()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
